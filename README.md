# GTM Semantic Crawler + PMF Laboratory

AI-powered GTM system that finds opportunities Expanso doesn't know to look for.

## The Problem

- Keyword monitoring only catches known terms
- Expanso is flexible, needs to find the right wedge for PMF
- Need to discover **problems**, not just track mentions

## Solution

This system semantically analyzes posts from HN and Reddit to identify people experiencing problems that Bacalhau/Expanso could solve, even if they never mention those terms.

## Quick Start

```bash
cd /home/daaronch/.openclaw/workspace/gtm-semantic

# Install dependencies
pip install -r requirements.txt

# Set API key (pick one)
export ANTHROPIC_API_KEY="your-key"  # preferred
# or
export OPENAI_API_KEY="your-key"

# Run full pipeline
python main.py full

# Or step by step
python main.py crawl      # Crawl HN + Reddit
python main.py analyze    # Run AI analysis
python main.py digest     # Generate digest
```

## CLI Usage

```bash
# Make CLI executable
chmod +x gtm

# Query opportunities
./gtm query --min-fit 7 --days 7

# Filter by use case
./gtm query -u ml_inference --min-fit 6

# Show stats
./gtm stats

# Show category trends
./gtm trends --days 30

# Export for outreach
./gtm export ml_inference --days 7 --limit 10

# Output as JSON
./gtm query --json-output > opportunities.json
```

## Architecture

```
gtm-semantic/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py      # Configuration, API keys, categories
â”œâ”€â”€ crawlers/
â”‚   â”œâ”€â”€ hn.py           # Hacker News (Algolia API)
â”‚   â””â”€â”€ reddit.py       # Reddit (JSON API)
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ __init__.py     # AI analysis pipeline
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema.sql      # SQLite schema
â”‚   â””â”€â”€ __init__.py     # Database operations
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ __init__.py     # Query interface
â”œâ”€â”€ digest/
â”‚   â””â”€â”€ __init__.py     # Daily digest generator
â”œâ”€â”€ main.py             # Orchestration
â””â”€â”€ gtm                 # CLI entry point
```

## How It Works

### 1. Crawlers
- **HN**: Searches Algolia API for semantic terms (not just keywords) âœ… Works
- **Reddit**: Monitors relevant subreddits (âš ï¸ requires API credentials - see note below)

### 2. AI Analysis Pipeline
For each post, the LLM answers:
- "Is this person experiencing a problem that edge computing / distributed data processing / Bacalhau could solve?"
- Fit score (0-10)
- Urgency score (0-10)
- Use case category
- Problem summary

Uses Claude Haiku or GPT-4o-mini for cost-effective volume processing.

### 3. Problem Taxonomy
Categories tracked:
- `ml_inference` - ML model deployment/inference at edge
- `data_pipelines` - ETL, data processing workflows
- `iot_edge` - IoT data processing, edge compute
- `batch_processing` - Large-scale batch jobs
- `distributed_compute` - General distributed needs
- `cost_optimization` - Cloud/compute cost reduction
- `latency_sensitive` - Low-latency requirements
- `data_locality` - Processing data where it lives
- `hybrid_cloud` - Multi-cloud infrastructure
- `workflow_orchestration` - Scheduling pain points

### 4. Daily Digest
- Top 10 opportunities by fit/urgency
- Category trends
- Emerging pattern alerts
- Sent to Telegram

## Reddit API Note

Reddit now blocks unauthenticated API requests. To enable Reddit crawling:

1. Create a Reddit app at https://www.reddit.com/prefs/apps
2. Get your client_id and client_secret
3. Set environment variables:
   ```bash
   export REDDIT_CLIENT_ID="your-id"
   export REDDIT_CLIENT_SECRET="your-secret"
   ```

Alternative: Use Pushshift API (if available) or focus on HN for initial PMF research.

## Configuration

Edit `config/settings.py`:

```python
# Model selection
ANALYSIS_MODEL = "claude-3-haiku-20240307"
ANALYSIS_PROVIDER = "anthropic"

# Subreddits to monitor
REDDIT_SUBREDDITS = [
    "dataengineering",
    "MachineLearning",
    "mlops",
    ...
]

# HN search terms
HN_SEARCH_TERMS = [
    "distributed computing",
    "edge computing",
    ...
]
```

## Cron Setup

Add to crontab for daily runs:

```bash
# Run at 6 AM UTC daily
0 6 * * * cd /home/daaronch/.openclaw/workspace/gtm-semantic && python main.py full >> /var/log/gtm-semantic.log 2>&1
```

## Database

SQLite database at `db/gtm_semantic.db`:

```sql
-- Query high-fit opportunities directly
SELECT p.title, p.url, a.fit_score, a.urgency_score, a.problem_summary
FROM posts p
JOIN analysis a ON p.id = a.post_id
WHERE a.fit_score >= 7
ORDER BY a.fit_score DESC, a.urgency_score DESC;
```

## Costs

Using Claude Haiku (~$0.00025 per 1K input tokens):
- ~500 posts/day analyzed
- ~$0.50-1.00/day estimated

## Output Examples

### Query
```
â”â”â”â”â”â”â”³â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Fit â”ƒ Urg â”ƒ Use Case        â”ƒ Source â”ƒ Summary                        â”ƒ
â”¡â”â”â”â”â”â•‡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚  9  â”‚  8  â”‚ ml_inference    â”‚ reddit â”‚ Struggling to deploy models... â”‚
â”‚  8  â”‚  7  â”‚ data_pipelines  â”‚ hn     â”‚ Airflow is too complex for...  â”‚
â”‚  8  â”‚  6  â”‚ iot_edge        â”‚ reddit â”‚ Need to process sensor data... â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Digest
```
ğŸ¯ GTM Semantic Digest
ğŸ“… 2024-02-06

ğŸ“Š Stats
â€¢ Total posts: 1,234
â€¢ Analyzed: 890
â€¢ High-fit opportunities (7+): 45

ğŸ”¥ Top Opportunities

1. [REDDIT] ml_inference (fit:9/urg:8)
   User struggling to deploy ML models at edge without cloud round-trip
   https://reddit.com/r/mlops/comments/...

ğŸ“ˆ Category Trends (7 days)
â€¢ ml_inference: 23 posts
â€¢ data_pipelines: 18 posts
â€¢ iot_edge: 12 posts

ğŸš¨ Emerging Patterns
â€¢ ğŸ“ˆ iot_edge is trending (+45% above expected)
```

## This Is Your PMF Laboratory

Use this to:
1. **Discover new use cases** - What problems keep appearing?
2. **Test messaging** - Which problem categories get highest fit scores?
3. **Find wedge opportunities** - Where is urgency highest?
4. **Track market evolution** - Are patterns changing?

The goal isn't just lead genâ€”it's understanding what problems the market actually has that you can solve.
